{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from rdkit import Chem\n",
    "import pickle\n",
    "\n",
    "from calc_property import calculate_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, data_path, data_length=None, shuffle=False):\n",
    "        with open(data_path,'r') as f:\n",
    "            lines = f.readlines()\n",
    "        self.data = [l.strip() for l in lines]\n",
    "\n",
    "        with open('./normalize.pkl', 'rb') as w:\n",
    "            norm = pickle.load(w)\n",
    "        self.property_mean, self.property_std = norm\n",
    "\n",
    "        if shuffle:\n",
    "            random.shuffle(self.data)\n",
    "        \n",
    "        ## Why need this line? ##\n",
    "        if data_length is not None:\n",
    "            self.data = self.data[data_length[0]: data_length[1]]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles = 'Q' + self.data[idx]\n",
    "        properties = (calculate_property(smiles[1:])-self.property_std) / self.property_mean\n",
    "        \n",
    "        return smiles, properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('QCN(c1ccccc1)c1ccccc1C(=O)NCC1(O)CCOCC1',\n",
       " tensor([  0.5682,   0.4030,   0.6777,   0.6901,   0.6560,   0.6904,   0.6890,\n",
       "           0.6158,   0.6544,   0.3731,   0.6623,  -1.0214,   0.5386,  -6.4296,\n",
       "           0.6370,   0.7405,   0.7696,   0.8429,   0.2686,   1.6701,   0.6852,\n",
       "           0.6222,   0.6510,   0.5670,   0.3475,   0.6824,   0.8768,   0.8768,\n",
       "          -0.1836,   2.4966,   0.1489,   0.7014,   0.6364,   0.2755,   0.4956,\n",
       "          -2.3909,   0.4995,   0.0625,   0.7835,  -1.1521,   0.3690,   0.4610,\n",
       "           0.6192,   0.3638, -29.1460,   0.2716,  -2.7021,   0.9223,   0.2647,\n",
       "           0.6883,   0.5710,   0.4035,   1.0784]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleDataset = SMILESDataset(data_path='./data/pubchem-1m-simple.txt', data_length=None, shuffle=False)\n",
    "sample = sampleDataset.__getitem__(0)\n",
    "\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 2.0210e+00,  7.5875e+02,  1.7544e+01,  1.4052e+01,  1.4723e+01,\n",
      "         1.1664e+01,  8.1625e+00,  8.8090e+00,  6.2020e+00,  6.9684e+00,\n",
      "         4.2730e+00,  4.9467e+00,  2.9353e+00,  3.5438e+00,  3.5288e+02,\n",
      "         1.1951e+00,  1.9034e+00,  2.5342e+00,  4.1357e-01, -2.1272e+00,\n",
      "         2.4343e+01,  3.3127e+02,  1.7561e+01,  7.6679e+00,  4.3503e+00,\n",
      "         1.4605e+02,  1.1184e+01,  1.1184e+01,  1.9147e-01, -9.6040e-01,\n",
      "         2.6799e+00,  9.4141e+01,  3.5333e+02,  1.7290e+00,  5.1585e+00,\n",
      "         2.6400e-01,  5.4614e-01,  8.1014e-01,  1.1892e+00,  7.1781e-01,\n",
      "         1.9070e+00,  4.0690e+00,  1.3547e+00,  6.2828e+00,  4.5800e-03,\n",
      "         5.4205e+00,  2.0371e-01,  3.9774e-01,  6.0145e-01,  1.2960e+02,\n",
      "         2.7171e+00,  6.8247e+01,  6.1739e-01]), tensor([5.9750e-01, 4.0613e+02, 5.8108e+00, 4.7454e+00, 4.7843e+00, 4.0368e+00,\n",
      "        2.9280e+00, 3.1273e+00, 2.3723e+00, 3.8311e+00, 1.8252e+00, 9.7076e+00,\n",
      "        1.4360e+00, 2.5803e+01, 1.1541e+02, 2.3501e-01, 2.9517e-01, 3.4393e-01,\n",
      "        2.3890e-01, 1.1825e+00, 8.3198e+00, 1.1012e+02, 6.0206e+00, 3.3283e+00,\n",
      "        2.6729e+00, 4.8121e+01, 2.9003e+00, 2.9004e+00, 2.1729e-01, 1.5170e+00,\n",
      "        2.3267e+00, 3.2254e+01, 1.1555e+02, 1.5236e+00, 2.4436e+00, 6.3120e-01,\n",
      "        7.2720e-01, 9.4935e-01, 1.0682e+00, 8.2700e-01, 1.2962e+00, 2.1244e+00,\n",
      "        1.1611e+00, 2.7145e+00, 1.3349e-01, 3.5276e+00, 5.5045e-01, 6.3316e-01,\n",
      "        8.4078e-01, 4.2797e+01, 1.4486e+00, 3.4261e+01, 2.1272e-01]))\n"
     ]
    }
   ],
   "source": [
    "with open('./data/pubchem-1m-simple.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "with open('./normalize.pkl', 'rb') as w:\n",
    "    norm = pickle.load(w)\n",
    "    print(norm)\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Custom Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\"./vocab_bpe_300.txt\" ,lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[PAD]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[UNK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CLS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[MASK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>##c12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>##[Si</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>##c(C(=O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>##[nH+]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>##(CC(=O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0       [PAD]\n",
       "1       [UNK]\n",
       "2       [CLS]\n",
       "3       [SEP]\n",
       "4      [MASK]\n",
       "..        ...\n",
       "295     ##c12\n",
       "296     ##[Si\n",
       "297  ##c(C(=O\n",
       "298   ##[nH+]\n",
       "299  ##(CC(=O\n",
       "\n",
       "[300 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_fwf('./vocab_bpe_300.txt', header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Q', '##CN', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[SEP]']\n",
      "[2, 5, 146, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(sample[0])\n",
    "print(encoded.tokens)\n",
    "print(encoded.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "## REGEX_PATTERN ##\n",
    "SMI_REGEX_PATTERN =  r\"(\\%\\([0-9]{3}\\)|\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\||\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>>?|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "\n",
    "class RegexTokenizer:\n",
    "    \"\"\"Run regex tokenization\"\"\"\n",
    "\n",
    "    def __init__(self, regex_pattern: str=SMI_REGEX_PATTERN) -> None:\n",
    "        \"\"\"Constructs a RegexTokenizer.\n",
    "        Args:\n",
    "            regex_pattern: regex pattern used for tokenization.\n",
    "            suffix: optional suffix for the tokens. Defaults to \"\".\n",
    "        \"\"\"\n",
    "        self.regex_pattern = regex_pattern\n",
    "        self.regex = re.compile(self.regex_pattern)\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Regex tokenization.\n",
    "        Args:\n",
    "            text: text to tokenize.\n",
    "        Returns:\n",
    "            extracted tokens separated by spaces.\n",
    "        \"\"\"\n",
    "        tokens = [token for token in self.regex.findall(text)]\n",
    "        return tokens\n",
    "    \n",
    "    \n",
    "class SMILESTokenizer(BertTokenizer):\n",
    "    def __init__(self, \n",
    "        vocab_file: str,\n",
    "        unk_token: str = \"[UNK]\",\n",
    "        sep_token: str = \"[SEP]\",\n",
    "        pad_token: str = \"[PAD]\",\n",
    "        cls_token: str = \"[CLS]\",\n",
    "        mask_token: str = \"[MASK]\",\n",
    "        do_lower_case = False,\n",
    "        **kwargs,\n",
    "        ) -> None:\n",
    "        \n",
    "        super().__init__(\n",
    "            vocab_file=vocab_file,\n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token,\n",
    "            pad_token=pad_token,\n",
    "            cls_token=cls_token,\n",
    "            mask_token=mask_token,\n",
    "            do_lower_case=do_lower_case,\n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "        self.tokenizer = RegexTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_config = {\n",
    "        'embed_dim': 256,#256\n",
    "        'batch_size': 4,#64\n",
    "        'temp': 0.07,\n",
    "        'queue_size': 2048,#65536\n",
    "        'momentum': 0.995,\n",
    "        'alpha': 0.4,\n",
    "        'bert_config': './config_bert.json',    #config file for BERT model. The configuration for ViT can be manually changed in albef.py\n",
    "        'schedular': {'sched': 'cosine', 'lr': 1e-4, 'epochs': 30, 'min_lr': 1e-5,\n",
    "                      'decay_rate': 1, 'warmup_lr': 1e-5, 'warmup_epochs': 20, 'cooldown_epochs': 0},\n",
    "        'optimizer': {'opt': 'adamW', 'lr': 1e-4, 'weight_decay': 0.02}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMILES Sequence tokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn \n",
    "from xbert import BertConfig \n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "\n",
    "class pretrain_SPMM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 tokenizer=None,\n",
    "                 config=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tokenizer = BertTokenizer('./vocab_bpe_300.txt', do_lower_case=False,do_basic_tokenize=False)\n",
    "        embed_dim = config['embed_dim']\n",
    "\n",
    "        smilesAndFusion_config = BertConfig.from_json_file('./config_bert_smiles_and_fusion_encoder.json')\n",
    "        property_config = BertConfig.from_json_file('./config_bert_property_encoder.json')\n",
    "        self.smilesEncoder = BertForMaskedLM(config = smilesAndFusion_config)\n",
    "        self.propertyEncoder = BertForMaskedLM(config = property_config)\n",
    "\n",
    "        smiles_width = self.smilesEncoder.config.hidden_size\n",
    "        property_width = self.propertyEncoder.config.hidden_size\n",
    "\n",
    "        self.smiles_proj = nn.Linear(smiles_width, embed_dim)\n",
    "        self.property_proj = nn.Linear(property_width, embed_dim)\n",
    "        \n",
    "        self.temp = nn.Parameter(torch.ones([]) * config['temp'])\n",
    "        self.queue_size = config['queue_size']\n",
    "        self.momentum = config['momentum']\n",
    "        \n",
    "        self.itm_head_smiles = nn.Linear(smiles_width, 2)\n",
    "        self.itm_head_properties = nn.Linear(property_width, 2)\n",
    "\n",
    "        # Momentum Model\n",
    "\n",
    "        self.smilesEncoder_m = BertForMaskedLM(config = smilesAndFusion_config)\n",
    "        self.propertyEncoder_m = BertForMaskedLM(config = property_config)\n",
    "        self.smiles_proj_m = nn.Linear(smiles_width, embed_dim)\n",
    "        self.property_proj_m = nn.Linear(property_width, embed_dim)\n",
    "\n",
    "        self.model_pairs = [[self.smilesEncoder, self.smilesEncoder_m],\n",
    "                            [self.smiles_proj, self.smiles_proj_m],\n",
    "                            [self.propertyEncoder, self.propertyEncoder_m],\n",
    "                            [self.property_proj, self.property_proj_m]]\n",
    "        \n",
    "        self.copy_params()\n",
    "\n",
    "        # Create the queue\n",
    "        self.register_buffer(\"smiles_queue\", torch.randn(embed_dim, self.queue_size))\n",
    "        self.register_buffer(\"text_queue\", torch.randn(embed_dim, self.queue_size))\n",
    "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
    "\n",
    "        self.image_queue = nn.functional.normalize(self.image_queue, dim=0)\n",
    "        self.text_queue = nn.functional.normalize(self.text_queue, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "    def forward():\n",
    "\n",
    "        #1. tokenize SMILES\n",
    "\n",
    "        #2. 50% MASK property vector\n",
    "\n",
    "        #3. input throug encoders\n",
    "\n",
    "        #4-1. Contrastive Loss between the different modalities\n",
    "\n",
    "        #4-2. Contrastive Loss within the same modalities\n",
    "\n",
    "        #5. X-attension\n",
    "\n",
    "        #6. Next property prediction\n",
    "\n",
    "        #7. Next word prediction\n",
    "\n",
    "        #8. SMILES property matching \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def copy_params(self):\n",
    "        for model_pair in self.model_pairs:\n",
    "            for param, param_m in zip(model_pair[0].parameters(), model_pair[1].parameters()):\n",
    "                param_m.data.copy_(param.data)  # initialize\n",
    "                param_m.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xbert import BertConfig \n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "smilesAndFusion_config = BertConfig.from_json_file('./config_bert_smiles_and_fusion_encoder.json')\n",
    "smilesEncoder = BertForMaskedLM(config = smilesAndFusion_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0400)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BISPL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01a600ee4c343e99426c2b6e5bbd26d22d733d8f4b38429737b3f68048676463"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
