{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sehui\\anaconda3\\envs\\BISPL\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from rdkit import Chem\n",
    "import pickle\n",
    "\n",
    "from calc_property import calculate_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, data_path, data_length=None, shuffle=False):\n",
    "        with open(data_path,'r') as f:\n",
    "            lines = f.readlines()\n",
    "        self.data = [l.strip() for l in lines]\n",
    "\n",
    "        with open('./normalize.pkl', 'rb') as w:\n",
    "            norm = pickle.load(w)\n",
    "        self.property_mean, self.property_std = norm\n",
    "\n",
    "        if shuffle:\n",
    "            random.shuffle(self.data)\n",
    "        \n",
    "        ## Why need this line? ##\n",
    "        if data_length is not None:\n",
    "            self.data = self.data[data_length[0]: data_length[1]]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles = 'Q' + self.data[idx]\n",
    "        properties = (calculate_property(smiles[1:])-self.property_std) / self.property_mean\n",
    "        \n",
    "        return smiles, properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('QCN(c1ccccc1)c1ccccc1C(=O)NCC1(O)CCOCC1',\n",
       " tensor([  0.5682,   0.4030,   0.6777,   0.6901,   0.6560,   0.6904,   0.6890,\n",
       "           0.6158,   0.6544,   0.3731,   0.6623,  -1.0214,   0.5386,  -6.4296,\n",
       "           0.6370,   0.7405,   0.7696,   0.8429,   0.2686,   1.6701,   0.6852,\n",
       "           0.6222,   0.6510,   0.5670,   0.3475,   0.6824,   0.8768,   0.8768,\n",
       "          -0.1836,   2.4966,   0.1489,   0.7014,   0.6364,   0.2755,   0.4956,\n",
       "          -2.3909,   0.4995,   0.0625,   0.7835,  -1.1521,   0.3690,   0.4610,\n",
       "           0.6192,   0.3638, -29.1460,   0.2716,  -2.7021,   0.9223,   0.2647,\n",
       "           0.6883,   0.5710,   0.4035,   1.0784]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleDataset = SMILESDataset(data_path='./data/pubchem-1m-simple.txt', data_length=None, shuffle=False)\n",
    "sample = sampleDataset.__getitem__(0)\n",
    "\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 2.0210e+00,  7.5875e+02,  1.7544e+01,  1.4052e+01,  1.4723e+01,\n",
      "         1.1664e+01,  8.1625e+00,  8.8090e+00,  6.2020e+00,  6.9684e+00,\n",
      "         4.2730e+00,  4.9467e+00,  2.9353e+00,  3.5438e+00,  3.5288e+02,\n",
      "         1.1951e+00,  1.9034e+00,  2.5342e+00,  4.1357e-01, -2.1272e+00,\n",
      "         2.4343e+01,  3.3127e+02,  1.7561e+01,  7.6679e+00,  4.3503e+00,\n",
      "         1.4605e+02,  1.1184e+01,  1.1184e+01,  1.9147e-01, -9.6040e-01,\n",
      "         2.6799e+00,  9.4141e+01,  3.5333e+02,  1.7290e+00,  5.1585e+00,\n",
      "         2.6400e-01,  5.4614e-01,  8.1014e-01,  1.1892e+00,  7.1781e-01,\n",
      "         1.9070e+00,  4.0690e+00,  1.3547e+00,  6.2828e+00,  4.5800e-03,\n",
      "         5.4205e+00,  2.0371e-01,  3.9774e-01,  6.0145e-01,  1.2960e+02,\n",
      "         2.7171e+00,  6.8247e+01,  6.1739e-01]), tensor([5.9750e-01, 4.0613e+02, 5.8108e+00, 4.7454e+00, 4.7843e+00, 4.0368e+00,\n",
      "        2.9280e+00, 3.1273e+00, 2.3723e+00, 3.8311e+00, 1.8252e+00, 9.7076e+00,\n",
      "        1.4360e+00, 2.5803e+01, 1.1541e+02, 2.3501e-01, 2.9517e-01, 3.4393e-01,\n",
      "        2.3890e-01, 1.1825e+00, 8.3198e+00, 1.1012e+02, 6.0206e+00, 3.3283e+00,\n",
      "        2.6729e+00, 4.8121e+01, 2.9003e+00, 2.9004e+00, 2.1729e-01, 1.5170e+00,\n",
      "        2.3267e+00, 3.2254e+01, 1.1555e+02, 1.5236e+00, 2.4436e+00, 6.3120e-01,\n",
      "        7.2720e-01, 9.4935e-01, 1.0682e+00, 8.2700e-01, 1.2962e+00, 2.1244e+00,\n",
      "        1.1611e+00, 2.7145e+00, 1.3349e-01, 3.5276e+00, 5.5045e-01, 6.3316e-01,\n",
      "        8.4078e-01, 4.2797e+01, 1.4486e+00, 3.4261e+01, 2.1272e-01]))\n"
     ]
    }
   ],
   "source": [
    "with open('./data/pubchem-1m-simple.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "with open('./normalize.pkl', 'rb') as w:\n",
    "    norm = pickle.load(w)\n",
    "    print(norm)\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Custom Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer(vocab_file= \"./vocab_bpe_300.txt\" ,lowercase=False, do_basic_tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[PAD]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[UNK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CLS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[MASK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>##c12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>##[Si</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>##c(C(=O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>##[nH+]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>##(CC(=O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0       [PAD]\n",
       "1       [UNK]\n",
       "2       [CLS]\n",
       "3       [SEP]\n",
       "4      [MASK]\n",
       "..        ...\n",
       "295     ##c12\n",
       "296     ##[Si\n",
       "297  ##c(C(=O\n",
       "298   ##[nH+]\n",
       "299  ##(CC(=O\n",
       "\n",
       "[300 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_fwf('./vocab_bpe_300.txt', header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rdkit import Chem\n",
    "# from transformers import BertTokenizer\n",
    "\n",
    "# from typing import List\n",
    "# import re\n",
    "\n",
    "# ## REGEX_PATTERN ##\n",
    "# SMI_REGEX_PATTERN =  r\"(\\%\\([0-9]{3}\\)|\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\||\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>>?|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "\n",
    "# class RegexTokenizer:\n",
    "#     \"\"\"Run regex tokenization\"\"\"\n",
    "\n",
    "#     def __init__(self, regex_pattern: str=SMI_REGEX_PATTERN) -> None:\n",
    "#         \"\"\"Constructs a RegexTokenizer.\n",
    "#         Args:\n",
    "#             regex_pattern: regex pattern used for tokenization.\n",
    "#             suffix: optional suffix for the tokens. Defaults to \"\".\n",
    "#         \"\"\"\n",
    "#         self.regex_pattern = regex_pattern\n",
    "#         self.regex = re.compile(self.regex_pattern)\n",
    "\n",
    "#     def tokenize(self, text: str) -> List[str]:\n",
    "#         \"\"\"Regex tokenization.\n",
    "#         Args:\n",
    "#             text: text to tokenize.\n",
    "#         Returns:\n",
    "#             extracted tokens separated by spaces.\n",
    "#         \"\"\"\n",
    "#         tokens = [token for token in self.regex.findall(text)]\n",
    "#         return tokens\n",
    "    \n",
    "    \n",
    "# class SMILESTokenizer(BertTokenizer):\n",
    "#     def __init__(self, \n",
    "#         vocab_file: str,\n",
    "#         unk_token: str = \"[UNK]\",\n",
    "#         sep_token: str = \"[SEP]\",\n",
    "#         pad_token: str = \"[PAD]\",\n",
    "#         cls_token: str = \"[CLS]\",\n",
    "#         mask_token: str = \"[MASK]\",\n",
    "#         do_lower_case = False,\n",
    "#         **kwargs,\n",
    "#         ) -> None:\n",
    "        \n",
    "#         super().__init__(\n",
    "#             vocab_file=vocab_file,\n",
    "#             unk_token=unk_token,\n",
    "#             sep_token=sep_token,\n",
    "#             pad_token=pad_token,\n",
    "#             cls_token=cls_token,\n",
    "#             mask_token=mask_token,\n",
    "#             do_lower_case=do_lower_case,\n",
    "#             **kwargs,\n",
    "#         )\n",
    "        \n",
    "#         self.tokenizer = RegexTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_config = {\n",
    "        'embed_dim': 256,#256\n",
    "        'property_width': 384, #???\n",
    "        'batch_size': 4,#64\n",
    "        'temp': 0.07,\n",
    "        'queue_size': 2048,#65536\n",
    "        'momentum': 0.995,\n",
    "        'alpha': 0.4,\n",
    "        'bert_config': './config_bert.json',    #config file for BERT model. The configuration for ViT can be manually changed in albef.py\n",
    "        'schedular': {'sched': 'cosine', 'lr': 1e-4, 'epochs': 30, 'min_lr': 1e-5,\n",
    "                      'decay_rate': 1, 'warmup_lr': 1e-5, 'warmup_epochs': 20, 'cooldown_epochs': 0},\n",
    "        'optimizer': {'opt': 'adamW', 'lr': 1e-4, 'weight_decay': 0.02}\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  2,   5, 146,   8, 212, 112, 115,  98, 222, 114,  98,   3]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "sample_SMILES = sample[0]\n",
    "sample_prop = sample[1]\n",
    "\n",
    "SMILES_token = tokenizer(sample_SMILES)\n",
    "input_ids = torch.LongTensor([SMILES_token['input_ids']])\n",
    "print(input_ids)\n",
    "attention_mask = torch.Tensor([SMILES_token['attention_mask']])\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xbert import BertConfig, BertForMaskedLM\n",
    "\n",
    "smilesAndFusion_config = BertConfig.from_json_file('./config_bert_smiles_and_fusion_encoder.json')\n",
    "smilesEncoder = BertForMaskedLM(config = smilesAndFusion_config)\n",
    "\n",
    "property_config = BertConfig.from_json_file('./config_bert_property_encoder.json')\n",
    "propertyEncoder = BertForMaskedLM(config = property_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1417, -0.4103, -0.3208,  ..., -1.1290, -0.5527, -0.0819],\n",
      "         [-0.2524,  0.3830, -0.2037,  ..., -0.3266,  0.0735, -0.4584],\n",
      "         [-0.1997,  0.0054, -0.2594,  ..., -0.7085, -0.2245, -0.2792],\n",
      "         ...,\n",
      "         [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
      "         [-0.1488, -0.3589, -0.3132,  ..., -1.0770, -0.5122, -0.1063],\n",
      "         [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "propertyOriginal = torch.rand([1,53])\n",
    "embedProperty = nn.Linear(1, 384)(propertyOriginal.unsqueeze(2))\n",
    "#print(embedProperty)\n",
    "\n",
    "property_CLS = nn.Parameter(torch.zeros([1,1,384]))\n",
    "property_MASK = nn.Parameter(torch.zeros([1,1,384]))\n",
    "property_MASK = property_MASK.expand(propertyOriginal.size(0), propertyOriginal.size(1), -1)\n",
    "\n",
    "halfMask = torch.bernoulli(torch.ones_like(propertyOriginal)*0.5)\n",
    "halfMaskBatch = halfMask.unsqueeze(2).repeat(1,1,property_MASK.size(2))\n",
    "maskedProperty = embedProperty*halfMaskBatch + property_MASK*halfMaskBatch\n",
    "\n",
    "print((embedProperty*halfMaskBatch))\n",
    "#print(property_MASK*(1-halfMaskBatch))\n",
    "\n",
    "inputProperty = torch.cat([property_CLS.expand(propertyOriginal.size(0), -1,-1), maskedProperty], dim=1)\n",
    "\n",
    "#print(inputProperty.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 54, 384])\n",
      "torch.Size([1, 12, 384])\n"
     ]
    }
   ],
   "source": [
    "encodedProp = propertyEncoder.bert(inputs_embeds=inputProperty, return_dict=True).last_hidden_state\n",
    "print(encodedProp.shape)\n",
    "encodedSmiles = smilesEncoder.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True, mode='text').last_hidden_state\n",
    "print(encodedSmiles.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0041, -0.8168,  0.5872,  ..., -0.3747, -0.2723,  0.1165],\n",
       "        [-0.0697,  1.3831, -0.2511,  ..., -2.2535,  1.1538,  0.6848],\n",
       "        [ 0.0629,  0.0897,  0.4163,  ...,  0.2974, -1.2093, -0.8941],\n",
       "        ...,\n",
       "        [ 0.0267, -1.2687, -1.6029,  ...,  0.4938,  1.7291,  0.1211],\n",
       "        [ 0.0428, -1.4156, -0.9930,  ...,  0.5963, -0.8524, -0.5831],\n",
       "        [ 0.0701,  0.8586, -0.7782,  ..., -0.2760, -0.9922, -0.1335]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F \n",
    "\n",
    "propertyProj = nn.Linear(384, 384)\n",
    "\n",
    "propertyAtts = torch.ones(encodedProp.size()[:-1], dtype=torch.long).to(inputProperty.device)\n",
    "propertyFeat = F.normalize(propertyProj(encodedProp[:,0,:]), dim=-1)\n",
    "\n",
    "property_queue = torch.randn(384, 2048)\n",
    "property_queue.clone().detach()\n",
    "\n",
    "propertyFeatAll = torch.cat([propertyFeat.t(), property_queue.clone().detach()], dim=1)\n",
    "propertyFeatAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0041, -0.8168,  0.5872,  ..., -0.3747, -0.2723,  0.1165],\n",
       "        [-0.0697,  1.3831, -0.2511,  ..., -2.2535,  1.1538,  0.6848],\n",
       "        [ 0.0629,  0.0897,  0.4163,  ...,  0.2974, -1.2093, -0.8941],\n",
       "        ...,\n",
       "        [ 0.0267, -1.2687, -1.6029,  ...,  0.4938,  1.7291,  0.1211],\n",
       "        [ 0.0428, -1.4156, -0.9930,  ...,  0.5963, -0.8524, -0.5831],\n",
       "        [ 0.0701,  0.8586, -0.7782,  ..., -0.2760, -0.9922, -0.1335]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smilesProj = nn.Linear(384, 384)\n",
    "smilesFeat = F.normalize(smilesProj(encodedSmiles[:,0,:]), dim=-1)\n",
    "\n",
    "smiles_queue = torch.randn(384, 2048)\n",
    "smiles_queue.clone().detach()\n",
    "\n",
    "smilesFeatAll = torch.cat([smilesFeat.t(), smiles_queue.clone().detach()], dim=1)\n",
    "propertyFeatAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2049])\n",
      "torch.Size([1, 2049])\n"
     ]
    }
   ],
   "source": [
    "sim_p2s = (propertyFeat @ smilesFeatAll)\n",
    "sim_targets = torch.zeros(sim_p2s.size()).to(propertyOriginal.device)\n",
    "print(sim_targets.shape)\n",
    "\n",
    "sim_s2p = smilesFeat @ propertyFeatAll\n",
    "sim_targets_same = torch.zeros(sim_s2p.size()).to(propertyOriginal.device)\n",
    "print(sim_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputProperty_pos = smilesEncoder.bert(encoder_embeds = encodedProp,\n",
    "                               attention_mask = propertyAtts,\n",
    "                               encoder_hidden_states = encodedSmiles,\n",
    "                               encoder_attention_mask = attention_mask,\n",
    "                               return_dict = True,\n",
    "                               mode= 'fusion'\n",
    "                               ).last_hidden_state[:,0,:]\n",
    "outputSmiles_pos = smilesEncoder.bert(encoder_embeds = encodedSmiles,\n",
    "                             attention_mask = attention_mask,\n",
    "                             encoder_hidden_states = encodedProp,\n",
    "                             encoder_attention_mask = propertyAtts,\n",
    "                             return_dict = True,\n",
    "                             mode= 'fusion'\n",
    "                             ).last_hidden_state[:,0,:]\n",
    "                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 384])\n",
      "torch.Size([1, 384])\n"
     ]
    }
   ],
   "source": [
    "print(outputProperty_pos.shape)\n",
    "print(outputSmiles_pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "weights_p2s = F.softmax(sim_p2s[:,:batch_size], dim=1)\n",
    "weights_s2p = F.softmax(sim_s2p[:,:batch_size], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1824, -0.0607,  1.3084,  ..., -1.4432,  0.4354,  2.1898],\n",
       "         [ 0.6177, -0.5268,  0.0507,  ..., -0.9035, -0.2982,  0.0671],\n",
       "         [ 0.4261,  0.9823, -0.1573,  ...,  0.0697,  0.6003,  0.0170],\n",
       "         ...,\n",
       "         [ 1.0068,  0.2572,  2.0727,  ..., -0.0098, -0.9208, -0.1247],\n",
       "         [ 0.6434, -0.1911,  0.1341,  ..., -0.7065, -0.5435,  0.2061],\n",
       "         [ 1.0247, -0.4561,  2.6050,  ..., -1.0719, -0.0312,  1.5180]]],\n",
       "       grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodedProp_neg = []\n",
    "neg_idx = torch.multinomial(weights_p2s[0], 1).item()\n",
    "encodedProp_neg.append(encodedProp[neg_idx])\n",
    "encodedProp_neg = torch.stack(encodedProp_neg, dim=0)\n",
    "encodedProp_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9357, -0.9966,  0.5626,  ...,  0.8034,  0.8001,  0.2025],\n",
       "         [ 0.0703,  0.1101, -1.6660,  ..., -0.8955,  0.3145,  1.6860],\n",
       "         [ 0.4842,  1.0346, -0.9396,  ..., -0.8054, -0.0582, -2.2141],\n",
       "         ...,\n",
       "         [ 2.2750,  0.9106, -2.1961,  ..., -0.2422,  0.9576,  0.2175],\n",
       "         [-0.6238, -0.0892,  0.5609,  ..., -0.0097,  1.5409,  0.3180],\n",
       "         [ 0.7790, -1.2032, -0.9792,  ...,  0.3504,  0.4163,  0.4745]]],\n",
       "       grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodedSmiles_neg = []\n",
    "smilesAtts_neg = []\n",
    "\n",
    "neg_idx = torch.multinomial(weights_s2p[0], 1).item()\n",
    "encodedSmiles_neg.append(encodedSmiles[neg_idx])\n",
    "smilesAtts_neg.append(attention_mask[neg_idx])\n",
    "encodedSmiles_neg = torch.stack(encodedSmiles_neg, dim=0)\n",
    "smilesAtts_neg = torch.stack(smilesAtts_neg, dim=0)\n",
    "\n",
    "encodedSmiles_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encProperty_all = torch.cat([encodedProp, encodedProp_neg], dim=0)\n",
    "propertyAtts_all = torch.cat([propertyAtts, propertyAtts], dim=0)\n",
    "\n",
    "encSmiles_all = torch.cat([encodedSmiles_neg, encodedSmiles], dim=0)\n",
    "smilesAtts_all = torch.cat([smilesAtts_neg, attention_mask], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputProperty_neg = smilesEncoder.bert(encoder_embeds = encProperty_all,\n",
    "                                    attention_mask = propertyAtts_all,\n",
    "                                    encoder_hidden_states = encSmiles_all,\n",
    "                                    encoder_attention_mask = smilesAtts_all,\n",
    "                                    return_dict = True,\n",
    "                                    mode = 'fusion'\n",
    "                                    ).last_hidden_state[:,0,:]\n",
    "#outputProperty_neg = outputProperty_neg\n",
    "outputSmiles_neg = smilesEncoder.bert(encoder_embeds = encSmiles_all,\n",
    "                                    attention_mask = smilesAtts_all,\n",
    "                                    encoder_hidden_states = encProperty_all,\n",
    "                                    encoder_attention_mask = propertyAtts_all,\n",
    "                                    return_dict = True,\n",
    "                                    mode = 'fusion'\n",
    "                                    ).last_hidden_state[:,0,:]\n",
    "#outputSmiles_neg = outputSmiles_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3131, -0.4291],\n",
       "        [ 0.3008, -0.5363],\n",
       "        [ 0.1552, -0.5555]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embeds = torch.cat([outputProperty_pos, outputSmiles_pos], dim=-1)\n",
    "neg_embeds = torch.cat([outputProperty_neg, outputSmiles_neg], dim=-1)\n",
    "\n",
    "ps_embeddings = torch.cat([pos_embeds, neg_embeds], dim=0)\n",
    "ps_embeddings.shape\n",
    "ps_output = nn.Linear(768, 2)(ps_embeddings)\n",
    "ps_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psm_labels = torch.cat([torch.ones(1, dtype=torch.long), torch.zeros(2, dtype=torch.long)], dim=0).to(propertyOriginal.device)\n",
    "psm_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6303, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_psm = F.cross_entropy(ps_output, psm_labels)\n",
    "loss_psm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True, False,  True,  True,  True,  True, False,  True,  True,\n",
       "          True, False]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bernoulli(torch.full(input_ids.shape, 0.8)).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 300])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_m  = smilesEncoder(input_ids,\n",
    "                          attention_mask = attention_mask,\n",
    "                          encoder_hidden_states = encodedProp,\n",
    "                          encoder_attention_mask = propertyAtts,\n",
    "                          return_dict = True,\n",
    "                          is_decoder = True,\n",
    "                          return_logits = True \n",
    "                          )[:,:-1,:]\n",
    "\n",
    "logits_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11, 300])\n",
      "torch.Size([1, 300, 11])\n",
      "torch.Size([1, 11])\n",
      "tensor(5.8192, grad_fn=<NllLoss2DBackward0>)\n"
     ]
    }
   ],
   "source": [
    "labels = input_ids.clone()[:,1:]\n",
    "per_logits_m = logits_m.permute((0,2,1))\n",
    "\n",
    "print(logits_m.shape)\n",
    "print(per_logits_m.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "loss_fct = nn.CrossEntropyLoss()\n",
    "loss_nwp = loss_fct(per_logits_m,labels)\n",
    "\n",
    "print(loss_nwp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0311, -0.0295, -0.0184,  ..., -0.0117, -0.0196, -0.0126],\n",
       "         [-0.0125, -0.0173, -0.0163,  ..., -0.0141, -0.0148, -0.0215],\n",
       "         [-0.0223, -0.0419, -0.0296,  ..., -0.0148, -0.0102, -0.0124],\n",
       "         ...,\n",
       "         [-0.0158, -0.0355, -0.0273,  ..., -0.0110, -0.0179, -0.0211],\n",
       "         [-0.0147, -0.0423, -0.0183,  ..., -0.0367, -0.0124, -0.0188],\n",
       "         [-0.0164, -0.0265, -0.0178,  ..., -0.0186, -0.0134, -0.0156]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.log_softmax(logits_m, dim=-1) * F.softmax(logits_m, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11, 300])\n"
     ]
    }
   ],
   "source": [
    "F.softmax(logits_m, dim=-1)\n",
    "print(F.softmax(logits_m, dim=-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(64.0643, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_labels = F.softmax(logits_m, dim=1)\n",
    "loss_distill = -torch.sum(F.log_softmax(logits_m, dim=1) * soft_labels, dim=-1)\n",
    "\n",
    "loss_distill = (loss_distill * (labels != 0)).mean()\n",
    "loss_distill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2711, -0.0187,  0.5958,  ..., -1.9072,  0.2913,  1.6481],\n",
       "         [ 0.7051, -1.0061,  0.1156,  ..., -0.8559,  0.0309,  0.1750],\n",
       "         [ 0.5251,  1.1804, -0.4049,  ...,  0.0625,  0.9561, -0.3600],\n",
       "         ...,\n",
       "         [ 1.4204,  0.1097,  1.9425,  ..., -0.2383, -0.9002, -0.3194],\n",
       "         [ 0.5641, -0.5436,  0.1660,  ..., -0.3512, -0.3763,  0.0207],\n",
       "         [ 1.0097, -0.0674,  2.3085,  ..., -1.1544, -0.1497,  1.2358]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = propertyOriginal.clone()\n",
    "\n",
    "encProperty_masked_m = propertyEncoder.bert(inputs_embeds = inputProperty,\n",
    "                                       return_dict = True,\n",
    "                                       is_decoder = True\n",
    "                                       ).last_hidden_state\n",
    "encProperty_masked_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 53, 384])\n"
     ]
    }
   ],
   "source": [
    "npp_output_m = smilesEncoder.bert(encoder_embeds = encProperty_masked_m,\n",
    "                                  attention_mask = propertyAtts,\n",
    "                                  encoder_hidden_states = encodedSmiles,\n",
    "                                  encoder_attention_mask = attention_mask,    \n",
    "                                  return_dict = True,\n",
    "                                  is_decoder = True,\n",
    "                                  mode = 'fusion').last_hidden_state[:,:-1,:]\n",
    "\n",
    "#npp_output_m\n",
    "print(npp_output_m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_reg = nn.Sequential(nn.Linear(384, 384),\n",
    "                           nn.GELU(),\n",
    "                           nn.LayerNorm(384, property_config.layer_norm_eps),\n",
    "                           nn.Linear(384, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 53])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_m = target_reg(npp_output_m)\n",
    "pred_m.squeeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 53])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propertyTargets = propertyOriginal.clone() \n",
    "propertyTargets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sehui\\anaconda3\\envs\\BISPL\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 53])) that is different to the input size (torch.Size([1, 53, 53])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.1642, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_MSE = nn.MSELoss()\n",
    "loss_npp = loss_MSE(pred_m*halfMask, propertyTargets*halfMask)\n",
    "loss_npp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
      "         0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
      "         1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0.]])\n",
      "tensor([[0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "         1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(halfMask)\n",
    "print(1-halfMask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BISPL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01a600ee4c343e99426c2b6e5bbd26d22d733d8f4b38429737b3f68048676463"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
