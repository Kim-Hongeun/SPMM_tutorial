{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sehui\\anaconda3\\envs\\BISPL\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from rdkit import Chem\n",
    "import pickle\n",
    "\n",
    "from calc_property import calculate_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, data_path, data_length=None, shuffle=False):\n",
    "        with open(data_path,'r') as f:\n",
    "            lines = f.readlines()\n",
    "        self.data = [l.strip() for l in lines]\n",
    "\n",
    "        with open('./normalize.pkl', 'rb') as w:\n",
    "            norm = pickle.load(w)\n",
    "        self.property_mean, self.property_std = norm\n",
    "\n",
    "        if shuffle:\n",
    "            random.shuffle(self.data)\n",
    "        \n",
    "        ## Why need this line? ##\n",
    "        if data_length is not None:\n",
    "            self.data = self.data[data_length[0]: data_length[1]]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles = 'Q' + self.data[idx]\n",
    "        properties = (calculate_property(smiles[1:])-self.property_std) / self.property_mean\n",
    "        \n",
    "        return smiles, properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('QCN(c1ccccc1)c1ccccc1C(=O)NCC1(O)CCOCC1',\n",
       " tensor([  0.5682,   0.4030,   0.6777,   0.6901,   0.6560,   0.6904,   0.6890,\n",
       "           0.6158,   0.6544,   0.3731,   0.6623,  -1.0214,   0.5386,  -6.4296,\n",
       "           0.6370,   0.7405,   0.7696,   0.8429,   0.2686,   1.6701,   0.6852,\n",
       "           0.6222,   0.6510,   0.5670,   0.3475,   0.6824,   0.8768,   0.8768,\n",
       "          -0.1836,   2.4966,   0.1489,   0.7014,   0.6364,   0.2755,   0.4956,\n",
       "          -2.3909,   0.4995,   0.0625,   0.7835,  -1.1521,   0.3690,   0.4610,\n",
       "           0.6192,   0.3638, -29.1460,   0.2716,  -2.7021,   0.9223,   0.2647,\n",
       "           0.6883,   0.5710,   0.4035,   1.0784]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleDataset = SMILESDataset(data_path='./data/pubchem-1m-simple.txt', data_length=None, shuffle=False)\n",
    "sample = sampleDataset.__getitem__(0)\n",
    "\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 2.0210e+00,  7.5875e+02,  1.7544e+01,  1.4052e+01,  1.4723e+01,\n",
      "         1.1664e+01,  8.1625e+00,  8.8090e+00,  6.2020e+00,  6.9684e+00,\n",
      "         4.2730e+00,  4.9467e+00,  2.9353e+00,  3.5438e+00,  3.5288e+02,\n",
      "         1.1951e+00,  1.9034e+00,  2.5342e+00,  4.1357e-01, -2.1272e+00,\n",
      "         2.4343e+01,  3.3127e+02,  1.7561e+01,  7.6679e+00,  4.3503e+00,\n",
      "         1.4605e+02,  1.1184e+01,  1.1184e+01,  1.9147e-01, -9.6040e-01,\n",
      "         2.6799e+00,  9.4141e+01,  3.5333e+02,  1.7290e+00,  5.1585e+00,\n",
      "         2.6400e-01,  5.4614e-01,  8.1014e-01,  1.1892e+00,  7.1781e-01,\n",
      "         1.9070e+00,  4.0690e+00,  1.3547e+00,  6.2828e+00,  4.5800e-03,\n",
      "         5.4205e+00,  2.0371e-01,  3.9774e-01,  6.0145e-01,  1.2960e+02,\n",
      "         2.7171e+00,  6.8247e+01,  6.1739e-01]), tensor([5.9750e-01, 4.0613e+02, 5.8108e+00, 4.7454e+00, 4.7843e+00, 4.0368e+00,\n",
      "        2.9280e+00, 3.1273e+00, 2.3723e+00, 3.8311e+00, 1.8252e+00, 9.7076e+00,\n",
      "        1.4360e+00, 2.5803e+01, 1.1541e+02, 2.3501e-01, 2.9517e-01, 3.4393e-01,\n",
      "        2.3890e-01, 1.1825e+00, 8.3198e+00, 1.1012e+02, 6.0206e+00, 3.3283e+00,\n",
      "        2.6729e+00, 4.8121e+01, 2.9003e+00, 2.9004e+00, 2.1729e-01, 1.5170e+00,\n",
      "        2.3267e+00, 3.2254e+01, 1.1555e+02, 1.5236e+00, 2.4436e+00, 6.3120e-01,\n",
      "        7.2720e-01, 9.4935e-01, 1.0682e+00, 8.2700e-01, 1.2962e+00, 2.1244e+00,\n",
      "        1.1611e+00, 2.7145e+00, 1.3349e-01, 3.5276e+00, 5.5045e-01, 6.3316e-01,\n",
      "        8.4078e-01, 4.2797e+01, 1.4486e+00, 3.4261e+01, 2.1272e-01]))\n"
     ]
    }
   ],
   "source": [
    "with open('./data/pubchem-1m-simple.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "with open('./normalize.pkl', 'rb') as w:\n",
    "    norm = pickle.load(w)\n",
    "    print(norm)\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Custom Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer(vocab_file= \"./vocab_bpe_300.txt\" ,lowercase=False, do_basic_tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[PAD]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[UNK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CLS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[MASK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>##c12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>##[Si</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>##c(C(=O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>##[nH+]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>##(CC(=O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0       [PAD]\n",
       "1       [UNK]\n",
       "2       [CLS]\n",
       "3       [SEP]\n",
       "4      [MASK]\n",
       "..        ...\n",
       "295     ##c12\n",
       "296     ##[Si\n",
       "297  ##c(C(=O\n",
       "298   ##[nH+]\n",
       "299  ##(CC(=O\n",
       "\n",
       "[300 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_fwf('./vocab_bpe_300.txt', header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rdkit import Chem\n",
    "# from transformers import BertTokenizer\n",
    "\n",
    "# from typing import List\n",
    "# import re\n",
    "\n",
    "# ## REGEX_PATTERN ##\n",
    "# SMI_REGEX_PATTERN =  r\"(\\%\\([0-9]{3}\\)|\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\||\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>>?|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "\n",
    "# class RegexTokenizer:\n",
    "#     \"\"\"Run regex tokenization\"\"\"\n",
    "\n",
    "#     def __init__(self, regex_pattern: str=SMI_REGEX_PATTERN) -> None:\n",
    "#         \"\"\"Constructs a RegexTokenizer.\n",
    "#         Args:\n",
    "#             regex_pattern: regex pattern used for tokenization.\n",
    "#             suffix: optional suffix for the tokens. Defaults to \"\".\n",
    "#         \"\"\"\n",
    "#         self.regex_pattern = regex_pattern\n",
    "#         self.regex = re.compile(self.regex_pattern)\n",
    "\n",
    "#     def tokenize(self, text: str) -> List[str]:\n",
    "#         \"\"\"Regex tokenization.\n",
    "#         Args:\n",
    "#             text: text to tokenize.\n",
    "#         Returns:\n",
    "#             extracted tokens separated by spaces.\n",
    "#         \"\"\"\n",
    "#         tokens = [token for token in self.regex.findall(text)]\n",
    "#         return tokens\n",
    "    \n",
    "    \n",
    "# class SMILESTokenizer(BertTokenizer):\n",
    "#     def __init__(self, \n",
    "#         vocab_file: str,\n",
    "#         unk_token: str = \"[UNK]\",\n",
    "#         sep_token: str = \"[SEP]\",\n",
    "#         pad_token: str = \"[PAD]\",\n",
    "#         cls_token: str = \"[CLS]\",\n",
    "#         mask_token: str = \"[MASK]\",\n",
    "#         do_lower_case = False,\n",
    "#         **kwargs,\n",
    "#         ) -> None:\n",
    "        \n",
    "#         super().__init__(\n",
    "#             vocab_file=vocab_file,\n",
    "#             unk_token=unk_token,\n",
    "#             sep_token=sep_token,\n",
    "#             pad_token=pad_token,\n",
    "#             cls_token=cls_token,\n",
    "#             mask_token=mask_token,\n",
    "#             do_lower_case=do_lower_case,\n",
    "#             **kwargs,\n",
    "#         )\n",
    "        \n",
    "#         self.tokenizer = RegexTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_config = {\n",
    "        'embed_dim': 256,#256\n",
    "        'property_width': 384, #???\n",
    "        'batch_size': 4,#64\n",
    "        'temp': 0.07,\n",
    "        'queue_size': 2048,#65536\n",
    "        'momentum': 0.995,\n",
    "        'alpha': 0.4,\n",
    "        'bert_config': './config_bert.json',    #config file for BERT model. The configuration for ViT can be manually changed in albef.py\n",
    "        'schedular': {'sched': 'cosine', 'lr': 1e-4, 'epochs': 30, 'min_lr': 1e-5,\n",
    "                      'decay_rate': 1, 'warmup_lr': 1e-5, 'warmup_epochs': 20, 'cooldown_epochs': 0},\n",
    "        'optimizer': {'opt': 'adamW', 'lr': 1e-4, 'weight_decay': 0.02}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMILES Sequence tokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn \n",
    "from xbert import BertConfig \n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "\n",
    "class SPMM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 tokenizer=None,\n",
    "                 config=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tokenizer = BertTokenizer('./vocab_bpe_300.txt', do_lower_case=False,do_basic_tokenize=False)\n",
    "        embed_dim = config['embed_dim']\n",
    "\n",
    "        smilesAndFusion_config = BertConfig.from_json_file('./config_bert_smiles_and_fusion_encoder.json')\n",
    "        property_config = BertConfig.from_json_file('./config_bert_property_encoder.json')\n",
    "        self.smilesEncoder = BertForMaskedLM(config = smilesAndFusion_config)\n",
    "        self.propertyEncoder = BertForMaskedLM(config = property_config)\n",
    "\n",
    "        smilesWidth = self.smilesEncoder.config.hidden_size\n",
    "        propertyWidth = config['property_width']\n",
    "\n",
    "        self.smilesProj = nn.Linear(smilesWidth, embed_dim)\n",
    "        self.propertyProj = nn.Linear(propertyWidth, embed_dim)\n",
    "\n",
    "        # special tokens & embedding for property input\n",
    "        self.propertyEmbed = nn.Linear(1, propertyWidth)\n",
    "        self.property_CLS = nn.parameter(torch.ones([1, 1, propertyWidth]))\n",
    "        self.property_MASK = nn.parameter(torch.ones([1, 1, propertyWidth]))\n",
    "        \n",
    "\n",
    "        self.temp = nn.Parameter(torch.ones([]) * config['temp'])\n",
    "        self.queue_size = config['queue_size']\n",
    "        self.momentum = config['momentum']\n",
    "        \n",
    "        self.itm_head_smiles = nn.Linear(smilesWidth, 2)\n",
    "        self.itm_head_properties = nn.Linear(propertyWidth, 2)\n",
    "\n",
    "        # Momentum Model\n",
    "\n",
    "        self.smilesEncoder_m = BertForMaskedLM(config = smilesAndFusion_config)\n",
    "        self.propertyEncoder_m = BertForMaskedLM(config = property_config)\n",
    "        self.smilesProj_m = nn.Linear(smilesWidth, embed_dim)\n",
    "        self.propertyProj_m = nn.Linear(propertyWidth, embed_dim)\n",
    "\n",
    "        self.model_pairs = [[self.smilesEncoder, self.smilesEncoder_m],\n",
    "                            [self.smilesProj, self.smilesProj_m],\n",
    "                            [self.propertyEncoder, self.propertyEncoder_m],\n",
    "                            [self.propertyProj, self.propertyProj_m]]\n",
    "        \n",
    "        self.copy_params()\n",
    "\n",
    "        # Create the queue\n",
    "        self.register_buffer(\"smiles_queue\", torch.randn(embed_dim, self.queue_size))\n",
    "        self.register_buffer(\"text_queue\", torch.randn(embed_dim, self.queue_size))\n",
    "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
    "\n",
    "        self.image_queue = nn.functional.normalize(self.image_queue, dim=0)\n",
    "        self.text_queue = nn.functional.normalize(self.text_queue, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, smiles, property, alpha=0):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.temp.clamp_(0.001, 0.5)\n",
    "\n",
    "        #1. property tokenizing & embedding\n",
    "        embedProperty = self.propertyEmbed(property.unsqueeze(2))\n",
    "        \n",
    "        property_MASK = self.property_MASK.expand(property.size(0), property.size(1), -1)\n",
    "        mask50 = torch.bernoulli(torch.ones_like(property)*0.5)\n",
    "        mask50Expand = mask50.unsqueeze(2).repeat(1,1,property_MASK.size(2))\n",
    "        maskedProperty = embedProperty*(1-mask50Expand) + property_MASK * mask50Expand\n",
    "        inputProperty = torch.cat([self.property_CLS.expand(property.size(0), -1,-1), maskedProperty], dim=-1)\n",
    "         \n",
    "        encodedProperty = self.propertyEncoder(inputs_embeds=property, return_dict=True).last_hidden_state\n",
    "\n",
    "        #2. input throug encoders\n",
    "        smilesEmbeds = self.smilesEncoder.bert(smiles, attention_mask = )\n",
    "        \n",
    "\n",
    "        #3-1. Contrastive Loss between the different modalities\n",
    "\n",
    "        #3-2. Contrastive Loss within the same modalities\n",
    "\n",
    "        #4. X-attention\n",
    "\n",
    "        #5. Next property prediction\n",
    "\n",
    "        #6. Next word prediction\n",
    "\n",
    "        #7. SMILES-property matching \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def copy_params(self):\n",
    "        for model_pair in self.model_pairs:\n",
    "            for param, param_m in zip(model_pair[0].parameters(), model_pair[1].parameters()):\n",
    "                param_m.data.copy_(param.data)  # initialize\n",
    "                param_m.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xbert import BertConfig \n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "smilesAndFusion_config = BertConfig.from_json_file('./config_bert_smiles_and_fusion_encoder.json')\n",
    "smilesEncoder = BertForMaskedLM(config = smilesAndFusion_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[0]\n",
    "sampleEmbed = tokenizer.encode(sample[0], padding='longest', truncation=True, max_length=100, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "propertyOriginal = torch.ones([32,53])\n",
    "torch.ones_like(propertyOriginal)*0.5\n",
    "\n",
    "mask50 = torch.bernoulli(torch.ones_like(propertyOriginal)*0.5)\n",
    "mask50Exp = mask50.unsqueeze(2).repeat(1,1,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 53, 10])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask50Exp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1].size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 213k/213k [00:00<00:00, 376kB/s]  \n",
      "c:\\Users\\sehui\\anaconda3\\envs\\BISPL\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sehui\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29.0/29.0 [00:00<00:00, 9.68kB/s]\n",
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 570/570 [00:00<00:00, 189kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer_sample = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence_a = \"This is a short sequence.\"\n",
    "sequence_b = \"This is a rather long sequence. It is at least longer than the sequence A.\"\n",
    "\n",
    "encoded_sequence_a = tokenizer_sample(sequence_a)[\"input_ids\"]\n",
    "encoded_sequence_b = tokenizer_sample(sequence_b)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1188, 1110, 170, 1897, 1263, 4954, 119, 1135, 1110, 1120, 1655, 2039, 1190, 1103, 4954, 138, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tokenizer_sample(sequence_b)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BISPL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01a600ee4c343e99426c2b6e5bbd26d22d733d8f4b38429737b3f68048676463"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
